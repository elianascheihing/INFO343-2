{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from  tqdm import tqdm_notebook\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npix = 28\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "P = np.random.permutation(10000)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    idx = P[i]\n",
    "    ax.matshow(np.reshape(mnist.train.images[idx, :], (npix, npix)), cmap=plt.cm.Greys_r)\n",
    "    ax.set_title(str(np.argmax(mnist.train.labels[idx])))\n",
    "    ax.axis('off')\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf_input = tf.placeholder(tf.float32, [None, 784])\n",
    "#z = tf.layers.conv2d(inputs=tf.reshape(tf_input, [-1, 28, 28, 1]), \n",
    "#                     kernel_size=3, strides=1, filters=10, use_bias=True, activation=tf.nn.relu)\n",
    "#z = tf.layers.max_pooling2d(inputs=z, pool_size=2, strides=2)\n",
    "#z = tf.layers.flatten(z)\n",
    "#z = tf.layers.dense(inputs=z, units=10, use_bias=True, activation=tf.nn.relu)\n",
    "y = tf.layers.dense(inputs=tf_input, units=10, use_bias=True, activation=tf.nn.softmax)\n",
    "tf_label = tf.placeholder(tf.float32, [None, 10])\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(tf_label * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(tf_label,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "nepochs = 20\n",
    "batch_size = 32\n",
    "metrics_train = np.zeros(shape=(nepochs, 2))\n",
    "metrics_test = np.zeros(shape=(nepochs, 2))\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0}) # USE CPU\n",
    "with tf.Session(config=config) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(nepochs), desc='Train epoch'):\n",
    "        for _ in range(int(mnist.train.images.shape[0]/batch_size)):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={tf_input: batch_xs, tf_label: batch_ys})\n",
    "        acc_value, loss_value = sess.run([accuracy, loss], \n",
    "                                         feed_dict={tf_input: mnist.train.images, \n",
    "                                                    tf_label: mnist.train.labels})\n",
    "        metrics_train[epoch, 0], metrics_train[epoch, 1] = acc_value, loss_value\n",
    "        acc_value, loss_value = sess.run([accuracy, loss], \n",
    "                                         feed_dict={tf_input: mnist.test.images, \n",
    "                                                    tf_label: mnist.test.labels})\n",
    "        metrics_test[epoch, 0], metrics_test[epoch, 1] = acc_value, loss_value\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for i, text in enumerate(['Accuraccy', 'Loss']):\n",
    "    ax = fig.add_subplot(1, 2, i+1)\n",
    "    ax.set_title(text)\n",
    "    ax.plot(metrics_train[:, i], linewidth=2, alpha=0.75, label='train')\n",
    "    ax.plot(metrics_test[:, i], linewidth=2, alpha=0.75, label='test')\n",
    "    plt.legend(); plt.grid();\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path_train = \"/home/phuijse/Work/Data/HiTS/snr_train.tfrecord\"\n",
    "path_val = \"/home/phuijse/Work/Data/HiTS/snr_validation.tfrecord\"\n",
    "\n",
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                                                 'label': tf.FixedLenFeature([], tf.int64),})\n",
    "    image = tf.decode_raw(features['image_raw'], tf.float32)\n",
    "    image = tf.reshape(image, [npix, npix, 4])\n",
    "    label = features['label']\n",
    "    return image, label\n",
    "\n",
    "def normalize_minmax(image, label):\n",
    "    image_max = tf.reduce_max(image, axis=[0, 1])\n",
    "    image_center = tf.reduce_min(image, axis=[0, 1])\n",
    "    image_scale  = tf.subtract(image_max, image_center)\n",
    "    image = tf.subtract(image, image_center)\n",
    "    image = tf.divide(image, tf.maximum(image_scale, tf.constant(1e-8, dtype=tf.float32)))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "\n",
    "npix = 21\n",
    "batch_size = 128\n",
    "n_batches = 100\n",
    "nepochs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_input = tf.placeholder(tf.float32, [None, npix, npix, 1], name='input')\n",
    "tf_label = tf.placeholder(tf.int64, None, name='target')\n",
    "\"\"\"\n",
    "with tf.variable_scope('conv_layer1'):\n",
    "    z = tf.layers.conv2d(inputs=tf_input, filters=64, kernel_size=5, strides=1, \n",
    "                         use_bias=True, activation=tf.nn.relu)\n",
    "    z = tf.layers.max_pooling2d(inputs=z, pool_size=2, strides=2)\n",
    "    \n",
    "with tf.variable_scope('conv_layer2'):\n",
    "    z = tf.layers.conv2d(inputs=z, filters=64, kernel_size=5, strides=2, \n",
    "                         use_bias=True, activation=tf.nn.relu)\n",
    "    z = tf.layers.max_pooling2d(inputs=z, pool_size=2, strides=2)\n",
    "\"\"\"\n",
    "with tf.variable_scope('fc_layer1'):\n",
    "    z = tf.layers.dense(tf.layers.flatten(tf_input), units=128, activation=tf.nn.tanh)\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    y = tf.layers.dense(z, units=1, activation=None)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    global_step_train = tf.Variable(0, name='global_step', trainable=False)  \n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(tf_label, tf.float32), \n",
    "                                                            logits=y)\n",
    "    loss_op = tf.reduce_mean(cross_entropy)  \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=global_step_train) \n",
    "    #init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    correct_prediction = tf.equal(tf_label, tf.argmax(tf.nn.sigmoid(y), 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "log_dir = str(time.time())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer_train = tf.summary.FileWriter(join(log_dir, 'train'), sess.graph)\n",
    "    writer_val = tf.summary.FileWriter(join(log_dir, 'val'), sess.graph)\n",
    "    \n",
    "    with tf.variable_scope('train_set'):\n",
    "        dataset_train = tf.data.TFRecordDataset(path_val)\n",
    "        dataset_train = dataset_train.map(decode)\n",
    "        dataset_train = dataset_train.map(normalize_minmax)\n",
    "        dataset_train = dataset_train.batch(batch_size)\n",
    "        dataset_train = dataset_train.shuffle(buffer_size=3*batch_size)\n",
    "        #dataset_train = dataset_train.repeat()\n",
    "        iterator_train = dataset_train.make_initializable_iterator()\n",
    "        next_element_train = iterator_train.get_next()\n",
    "        \n",
    "    with tf.variable_scope('val_set'):\n",
    "        dataset_val = tf.data.TFRecordDataset(path_val)\n",
    "        dataset_val = dataset_val.map(decode)\n",
    "        dataset_val = dataset_val.map(normalize_minmax)\n",
    "        dataset_val = dataset_val.batch(10000)\n",
    "        #dataset_val = dataset_val.repeat()\n",
    "        iterator_val = dataset_val.make_initializable_iterator()\n",
    "        next_element_val = iterator_val.get_next()\n",
    "    \n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for k in tqdm_notebook(range(nepochs), desc='Train epoch'):\n",
    "        sess.run(iterator_train.initializer)\n",
    "        for batch_index in range(n_batches): # Train\n",
    "            img, lbl = sess.run(next_element_train)\n",
    "            img = img[:, :, :, 3, np.newaxis]\n",
    "            _, summaries, step = sess.run([train_op, merged, global_step_train], \n",
    "                                          feed_dict={tf_input: img, tf_label: lbl})\n",
    "            writer_train.add_summary(summaries, global_step=step)\n",
    "        sess.run(iterator_val.initializer)\n",
    "        img, lbl = sess.run(next_element_val)\n",
    "        img = img[:, :, :, 3, np.newaxis]\n",
    "        _, summaries, acc = sess.run([loss_op, merged, accuracy], \n",
    "                                     feed_dict={tf_input: img, tf_label: lbl})\n",
    "        print(\"%d %f\" %(k, acc))\n",
    "        writer_val.add_summary(summaries, global_step=k)\n",
    "        y_val = sess.run(y, feed_dict={tf_input: img, tf_label: lbl})\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_images(img, lbl, n_images=10):\n",
    "    fig = plt.figure(figsize=(4, n_images))\n",
    "    labels = ['Template', 'Science', 'Diff', 'SNR']\n",
    "    for k1 in range(n_images):\n",
    "        for k2 in range(4):\n",
    "            ax = fig.add_subplot(n_images, 4, 4*k1 + k2+1)\n",
    "            ax.matshow(img[k1,:, :, k2], cmap=plt.cm.Greys_r)\n",
    "            ax.axis('off')\n",
    "            if k1 == 0:\n",
    "                ax.set_title(labels[k2])\n",
    "plot_images(img, lbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
