{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy_xor(N_per_cluster=500, stddev_noise=0.4):\n",
    "    data = stddev_noise*np.random.randn(4*N_per_cluster, 2)\n",
    "    data[0*N_per_cluster:1*N_per_cluster, :] += [1.0, -1.0]\n",
    "    data[1*N_per_cluster:2*N_per_cluster, :] += [-1.0, 1.0]\n",
    "    data[2*N_per_cluster:3*N_per_cluster :] += [-1.0, -1.0]\n",
    "    data[3*N_per_cluster:4*N_per_cluster, :] += [1.0, 1.0]\n",
    "    #data = (data - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    labels = np.zeros(shape=(4*N_per_cluster,), dtype=int)\n",
    "    labels[2*N_per_cluster:] = 1.0\n",
    "    NP = np.random.permutation(4*N_per_cluster)\n",
    "    return data[NP, :], labels[NP]\n",
    "\n",
    "# Create training and test set\n",
    "data , labels = create_noisy_xor()\n",
    "data_train, labels_train = data[:500, :], labels[:500]\n",
    "data_test, labels_test = data[500:, :], labels[500:]\n",
    "print(\"%d samples for train and %d for testing\" %(len(data_train), len(data)-len(data_train)))\n",
    "# Plot data\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], marker='o', label=\"class 1\", alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], marker='o', label=\"class 2\", alpha=0.5)\n",
    "ax.grid()\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=10, rstate=None):\n",
    "        np.random.seed(rstate)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        output_dim = 1 # Only for binary classification or single variable regression\n",
    "        if hidden_dim > 0:\n",
    "            self.wh = np.random.randn(input_dim, hidden_dim)\n",
    "            self.bh = np.random.randn(hidden_dim)\n",
    "            self.wo = np.random.randn(hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.wo = np.random.randn(input_dim, output_dim)\n",
    "        self.bo = np.random.randn(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hidden_dim > 0:\n",
    "            z = np.tanh(np.dot(x, self.wh) + self.bh)\n",
    "        else:\n",
    "            z = x\n",
    "        return logistic(np.dot(z, self.wo) + self.bo)\n",
    "        \n",
    "    def score(self, x, y):        \n",
    "        ypred = self.forward(x)[:, 0] # Binary classification\n",
    "        #return 0.5*np.power(y - ypred, 2)\n",
    "        return -y*np.log(ypred) - (1.0-y)*np.log(1.0-ypred)\n",
    "    \n",
    "    def backward(self, x, y, eta=1e-2):\n",
    "        ypred = self.forward(x)\n",
    "        #error = -(y - ypred)  # MSE plus linear activation\n",
    "        #error = -(y - ypred)*ypred*(1.0 - ypred)  # MSE plus sigmoid activation\n",
    "        error = -(y - ypred)  # Cross-entropy plus sigmoid activation\n",
    "        dbo = np.sum(error, axis=0)        \n",
    "        if self.hidden_dim > 0:\n",
    "            z_linear = np.dot(x, self.wh) + self.bh\n",
    "            dwo = np.dot(np.tanh(z_linear).T, error)\n",
    "            grad_z = (1.0 - np.tanh(z_linear)**2)\n",
    "            dbh = np.dot(grad_z.T, error)[:, 0]\n",
    "            dwh = np.dot(x.T, error*grad_z)\n",
    "            self.wh -= eta*dwh\n",
    "            self.bh -= eta*dbh\n",
    "        else:\n",
    "            dwo = np.dot(x.T, error)\n",
    "        self.bo -= eta*dbo\n",
    "        self.wo -= eta*dwo\n",
    "\n",
    "# Modifique la red para hacer regresión en ves de clasificación\n",
    "# Modifique la red para usar sigmoide en vez de tangente hiperbólica en la capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = MLP(hidden_dim=1)\n",
    "cost_history_train = list()\n",
    "cost_history_test = list()\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch training, only one epoch\n",
    "for epoch in range(1):\n",
    "    for i in range(len(data_train)//10):\n",
    "        nnet.backward(data_train[i*10:(i+1)*10, :], labels_train[i*10:(i+1)*10, np.newaxis], eta=1e-2)\n",
    "    \n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "x_min, x_max = data[:, 0].min() - 0.5, data[:, 0].max() + 0.5\n",
    "y_min, y_max = data[:, 1].min() - 0.5, data[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "Z = nnet.forward(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75)\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "cost_history_train.append(np.mean(nnet.score(data_train, labels_train)))\n",
    "cost_history_test.append(np.mean(nnet.score(data_test, labels_test)))\n",
    "ax.plot(np.arange(0, k, step=1), cost_history_train, '-o', label=\"train MSE\")\n",
    "ax.plot(np.arange(0, k, step=1), cost_history_test, '-o', label=\"test MSE\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "- Experimente variando el número de capas, número de neuronas y tasa de aprendizaje. Comente sobre como se reflejan estas modificaciones en el desempeño de la red y en la  complejidad del hiperplano\n",
    "- Experimente aumentando el ruido de los datos\n",
    "- Discuta sobre la relación entre complejidad del hiperplano, capacidad de generalización y sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación usando tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from os.path import join\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf_input = tf.placeholder(tf.float32, [None, 2], name='input')\n",
    "tf_label = tf.placeholder(tf.float32, [None, 1], name='target')\n",
    "\n",
    "Nh = 10\n",
    "nepochs = 500  \n",
    "\n",
    "with tf.variable_scope('Hidden_layer'):\n",
    "    bh = tf.Variable(tf.zeros([Nh]), name=\"bias\", dtype=tf.float32)\n",
    "    wh = tf.Variable(tf.random_uniform([2, Nh], -1.0, 1.0), name=\"weight\", dtype=tf.float32)\n",
    "    z = tf.nn.tanh(tf.matmul(tf_input, wh) + bh)\n",
    "\n",
    "with tf.variable_scope('Output_layer'):\n",
    "    bo = tf.Variable(tf.zeros([1]), name=\"bias\", dtype=tf.float32)\n",
    "    wo = tf.Variable(tf.random_uniform([Nh, 1], -1.0, 1.0), name=\"weight\", dtype=tf.float32)\n",
    "    y = tf.add(tf.matmul(z, wo), bo)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_label, logits=y)\n",
    "    loss_op = tf.reduce_mean(cross_entropy)  \n",
    "    optimizer = tf.train.AdamOptimizer(1e-2)\n",
    "    train_op = optimizer.minimize(loss_op) \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    tf.summary.histogram('output_weight', wo)\n",
    "    tf.summary.histogram('output_bias', bo)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios:** \n",
    "- Visualice el grafo, las curvas de aprendizaje y los histogramas de parámetros usando la herramienta tensorboard\n",
    "- Modifique el código que genera el grafo para agregar una segunda capa oculta\n",
    "- Estudie la función de mayor abstracción tf.layers.dense y usela para modificar el código que genera el grafo\n",
    "- ¿Cómo modificaría el código de entrenamiento para usar mini-batches?\n",
    "\n",
    "**Instrucciones tensorboard**\n",
    "1. Ejecutar: tensorboard --logdir /tmp/tensorboard/ \n",
    "2. Apuntar el navegador a localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = join(\"/tmp/tensorboard/\", str(time.time()))\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(join(log_dir, 'train'), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(join(log_dir, 'test'), sess.graph)\n",
    "    sess.run(init)\n",
    "    train_loss = np.zeros(shape=(nepochs))\n",
    "    test_loss = np.zeros(shape=(nepochs))\n",
    "    for i, epoch in enumerate(range(nepochs)):\n",
    "        # run the training operation\n",
    "        _, train_loss[i], summary = sess.run([train_op, loss_op, merged], feed_dict={tf_input: data_train, \n",
    "                                                         tf_label: np.reshape(labels_train, [-1, 1])})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        pred_test, test_loss[i], summary = sess.run([y, loss_op, merged], feed_dict={tf_input: data_test, \n",
    "                                                                 tf_label: np.reshape(labels_test, [-1, 1])})\n",
    "        test_writer.add_summary(summary, i)\n",
    "\n",
    "    Z = sess.run(y, feed_dict={tf_input: (np.c_[xx.ravel(), yy.ravel()]).astype('float32')})\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75)\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_loss, label='Train loss', linewidth=2)\n",
    "ax.plot(test_loss, label='Test loss', linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorboard: análisis del grafo, curvas de aprendizaje, histograma de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación usando pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Modifique la clase para agregando una segunda capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "class myMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_input, dim_hidden=50, dim_classes=1):        \n",
    "        \n",
    "        super(myMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(in_features=dim_input,  out_features=dim_hidden, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(in_features=dim_hidden, out_features=dim_classes, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = F.tanh(self.fc1(x))\n",
    "        y = self.fc2(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Modifique el siguiente código para correr en GPU. \n",
    "\n",
    "Por qué no se ve un speed-up considerable en este caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tqdm import tqdm_notebook\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch_train_set = TensorDataset(torch.from_numpy(data_train.astype('float32')), \n",
    "                                torch.from_numpy(labels_train.astype('float32'))\n",
    "                                )\n",
    "torch_train_loader = DataLoader(torch_train_set, shuffle=True, batch_size=32)\n",
    "\n",
    "net = myMLP(dim_input=2, dim_hidden=10, dim_classes=1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#net.to(device)\n",
    "\n",
    "# Main training loop\n",
    "nepoch = 500\n",
    "running_loss = np.zeros(shape=(nepoch,))\n",
    "for k in tqdm_notebook(range(nepoch), desc='Epochs'): \n",
    "    for sample_data, sample_label in torch_train_loader:\n",
    "        output = net(sample_data)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(output[:, 0], sample_label)  \n",
    "        running_loss[k] += loss.item()/torch_train_loader.__len__()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = net.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "Z = Z.detach().numpy().reshape(xx.shape)\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75)\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(running_loss, label='Train loss', linewidth=2)\n",
    "#ax.plot(test_loss, label='Test loss', linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
