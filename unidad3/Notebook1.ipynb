{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unidad 3: Datos textuales de medios de prensa y clasificación\n",
    "\n",
    "<h1> Notebook 1 - Introducción al tratamiento automático del lenguaje humano </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es el lenguaje humano?\n",
    "\n",
    "- Es un proceso de __comunicación__ entre humanos basado sobre un sistema complejo de signos\n",
    "\n",
    "- Es un proceso de __percepción__ del mundo\n",
    "\n",
    "¿Es posible pensar el mundo fuera del lenguaje? (ej: concepto de neolengua de George Orwell en la novela 1984, discursos póliticos y manipulación)\n",
    "\n",
    "- El lenguaje humano puede ser __verbal__ o __no verbal__\n",
    "\n",
    "- El lenguaje verbal puede ser __oral__ o __escrito__\n",
    "\n",
    "- Los humanos utilizan el lenguaje para cumplir 6 grandes tipos de función:\n",
    "    1. _expresar su subjetividad_ (opiniones, emociones, creencias, etc.). Ej: \"¡Me gusta la música!\", \"soy mejor que tú\", etc.\n",
    "    1. _solicitar que el interlocutor exprese su subjetividad_. Ej: \"¿Qué piensas?\", \"¿te \"gusta el deporte?\", etc.\n",
    "    1. _describir el mundo_. Ej: \"Hay una mesa y 4 sillas\", etc.\n",
    "    1. _activar o mantener la comunicación_. Ej: \"Hola\", \"Allo\", \"mmm\", \"eeeh\"\n",
    "    1. _ponerse de acuerdo sobre el sentido de un signo_. Ej: \"Hace frio significa que la temperatura es bajo 10°c\"\n",
    "    1. _comunicar por el placer de comunicar_. Ej: poesia, juegos de palabras, bromas lingúïsticas, etc.\n",
    "    \n",
    "\n",
    "- En conclusión, el lenguaje humano siempre se inscribe en una __situación de comunicación__, revela __explicatamente__ o __implicitamente__ el objetivo de comunicacion un locutor, dentro de un contexto particular. \n",
    "\n",
    "Ejemplo: <code>\"¿Viste como llueve?\"</code>\n",
    "\n",
    "¿Es una simple descripción del mundo o expresión de subjetividad para decir en realidad: \"¡deberías tomar tu paragua!\"?\n",
    "\n",
    "- El lenguaje humano tiene distinto nivel de interpretación:\n",
    "    - nivel __lexico-semántico__ : significado las palabras utilizadas.\n",
    "    - nivel __pragmático-discursivo__ : significado de estas palabras en su contexto.\n",
    "\n",
    "<code>\"Hemos realizados intervenciones quirúrgicas en afganistán\"</code>\n",
    "\n",
    "Busca significar que las intervenciones fueron las más limpias y precisas posibles, pero tambien busca ocultar que se trata de bombardeos y de violencia.\n",
    "       \n",
    "\n",
    "## 2. ¿Qué el Tratamiento Automático del Lenguaje (o _NLP_)?\n",
    "\n",
    "- Es una sub-disciplina de la Informática y de la Inteligencia Artificial que busca dotar los computadores de capacidad para entender, traducir y generar lenguaje humano a través de algoritmos y datos.\n",
    "\n",
    "- Es una disciplina antigua pero creciente dado el desarrollo de la comunicación en Internet y el desarrollo de las técnicas de Machine Learning/Deep Learning.\n",
    "\n",
    "- Problemas particulares del TAL en comparación con otras areas: __datos no estructurados__, __muchas(!!!) ambiguedades en los datos__, __problemas de grandes dimensiones__ (muchas variables posibles).\n",
    "\n",
    "- Tareas clásicas: Traducción Automática, Question-Answering, Análisis de opiniones y sentimientos, Extracción de información, Análisis del discurso, etc.\n",
    "\n",
    "- Tareas clásicas para el lenguaje oral: reconocimiento de las palabras, reconocimiento del locutor, etc.\n",
    "\n",
    "- Tarea para el lenguaje no verbal: reconocimiento des las emociones, reconocimiento de gestos, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recordatorio: manipulación de _ String_ en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hola Mundo. Bienvenido a la Unidad 3 - Notebook 1\"\n",
    "\n",
    "print(\"El cuarto caracter del texto es: \"+text[3])\n",
    "\n",
    "print(\"El tamaño del texto es: \"+str(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de caracteres 'o' en el texto :\"+str(text.count(\"o\")))\n",
    "\n",
    "print(\"El próximo caracter 'o' se encuentra en posición :\"+ str(text.find(\"o\")))\n",
    "\n",
    "print(\"La palabra 'Mundo' aparece a partir de la posición :\" + str(text.index(\"Mundo\")))\n",
    "\n",
    "print(\"¿Cuál es el texto entre la posición 12 y 22? : \"+str(text[12:22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convertimos el texto en _lower case_ : \"+ text.lower())\n",
    "\n",
    "print(\"Convertimos el texto en _lower case_ : \"+ text.upper())\n",
    "\n",
    "print(\"Convertimos el texto poniendo una mayúscula solamente en la primera palabra: \"+ text.capitalize())\n",
    "\n",
    "print(\"El texto empieza por un 'H': \"+str(text.startswith('H')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Replace 'Mundo' por 'Pizza': \"+str(text.replace(\"Mundo\",\"Pizza\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar si una variable contiene solamente digitos o caracteres\n",
    "\n",
    "text1=\"123\"\n",
    "text2=\"Hola\"\n",
    "\n",
    "print(text1.isdigit())\n",
    "print(text1.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un carácter de escape consiste en una barra invertida (\\) seguida del carácter que desea añadir a la cadena. \n",
    "\n",
    "print(\"Hello there!\\nHow are you?\\nI\\'m doing fine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Introducción al preprocesamiento de textos con spaCy\n",
    "\n",
    "### 4.1 Tokenización, Stop-Words y Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargaremos la librería de NLP spaCy (https://spacy.io/) y los modelos para procesar textos en español.\n",
    "\n",
    "<code>pip3 install -U spacy</code>\n",
    "\n",
    "<code>python3 -m spacy download es_core_news_sm</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Donald Trump es el presidente de Estados Unidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pipeline.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.is_stop, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El concepto de 'stop words' no tiene una definición objetiva. Una palabra 'stop words' depende mucho del caso de uso. Habitualmente se trata de una lista de palabras gramaticales tales como \"es\", \"el\", \"la\", \"quiere\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [u'presidente']\n",
    "for stopword in my_stop_words:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "    \n",
    "my_non_stop_words = [u'Estados']\n",
    "for nonstopword in my_non_stop_words:\n",
    "    lexeme = nlp.vocab[nonstopword]\n",
    "    lexeme.is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.is_stop, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Clasificación de la categoría gramatical de las palabras (POS tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Reconocimiento de los nombres de entidades (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Amazon tiene oficinas en todos los paises de America del Sur.')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, spaCy utiliza define varios tipos de entidades entre los cuales:\n",
    "- PERSON: personas\n",
    "- ORG: organizaciones, empresas, instituciones, etc.\n",
    "- GPE: paises, ciudades, regiones.\n",
    "- LOC: lugares geografícos que no son paises, ciudades o regiones.\n",
    "- PRODUCT: productos\n",
    "- EVENT: eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otros preprocesamientos posibles en el paquete spaCy: https://spacy.io/usage/linguistic-features\n",
    "- Dependency parsing\n",
    "- Sentence segmentation\n",
    "- Rule-based matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Crear representaciones vectoriales de los textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Modelo _Bag of words_\n",
    "\n",
    "Avanzamos hacia la parte de aprendizaje automático del análisis de texto. Empezaremos a jugar un poco menos con palabras y un poco más con números. Representaremos los textos como __vectores__.\n",
    "\n",
    "Los algoritmos de aprendizaje automático utilizan estos vectores en particular para hacer predicciones (algoritmos de aprendizaje supervisado) o agrupamiento (no supervisado).\n",
    "\n",
    "La manera la más clásica de representar un texto como un vector es el modelo __Bag of Words__.\n",
    "\n",
    "Empecemos con dos frases de ejemplo:\n",
    "\n",
    "F1:<code>El gato juega con el perro</code>,\n",
    "F2:<code>El perro duerme</code>\n",
    "    \n",
    "Si aplicamos los preprocesos que vimos antes, llegamos a:\n",
    "\n",
    "F1:<code>gato jugar perro</code>,\n",
    "F2:<code>perro dormir</code>\n",
    "\n",
    "Si queremos representar esto como un vector, necesitaríamos primero construir nuestro vocabulario, que serían las palabras únicas que se encuentran en las oraciones. \n",
    "\n",
    "<code>Vocab = ['gato', 'jugar', 'dormir', 'perro']</code>\n",
    "\n",
    "Y luego representar las frases con la frecuencia de aparición de cada palabra:\n",
    "\n",
    "F1:<code>[1,1,0,1]</code>\n",
    "F2:<code>[0,0,1,1]</code>\n",
    "\n",
    "##### Limitaciones del modelo _Bag of words_\n",
    "\n",
    "Como pueden darse cuenta, en el modelo Bag of words, se pierde el orden de las palabras y entonces se pierde parte del sentido del texto. Sin embargo, para muchas tareas de clasificación automática el modelo Bag of words es suficiente (ej: detección de spam).\n",
    "\n",
    "Otra limitación de lo visto hasta ahora es que estamos suponiendo que cada palabra tiene la misma importancia para revelar el sentido del texto. Detallamos esta idea en la sección siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Representar la importancia de una palabra: TF-IDF\n",
    "\n",
    "TF-IDF es la abreviatura de _Term Frequency_ (frecuencia de término) - _Inverse Document Frequency_ (frecuencia de documento inversa). Ampliamente utilizado en los motores de búsqueda para encontrar documentos relevantes basados en una consulta, es un enfoque bastante intuitivo para convertir nuestras frases en vectores.\n",
    "\n",
    "Como su nombre indica, TF-IDF trata de combinar dos tipos diferentes de información:\n",
    "- la frecuencia de término (TF) es el número de veces que una palabra aparece en un documento dividido por el número de palabras en el texto. Mide la importancia local del término en el texto.\n",
    "\n",
    "- IDF es la fracción inversa a escala logarítmica de los documentos que contienen la palabra. \n",
    "IDF(t) = log_e (total number of documents / number of documents with term t in it)\n",
    "La intuición \n",
    "\n",
    "TF-IDF es simplemente el producto de estos dos factores - TF e IDF. Juntos, encapsulan más información en la representación vectorial, en lugar de limitarse a utilizar el recuento de palabras como en la representación vectorial de la bolsa de palabras. TF-IDF hace que las palabras raras sean más relevantes para representar el sentido del texto.\n",
    "\n",
    "Si tomamos nuestras frases de ejemplo, tendriamos los vectores siguientes:\n",
    "F1:<code>[0.1,0.1,0,0]</code>\n",
    "F2:<code>[0,0,0.15,0]</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ejemplo de preprocesamiento y vectorización de un dataset real\n",
    "\n",
    "### 6.1 Dataset (o _Corpus_)\n",
    "\n",
    "Utilizaremos un dataset de textos en español que corresponde a una muestra de 1.000 noticias publicadas en 2018 por el medio La Tercera (http://www.latercera.com), recopiladas a traves de la herramienta Sophia (http://www.sophia-project.cl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_CSV=\"sophia_latercera-1000.csv\"\n",
    "\n",
    "df = pd.read_csv(DATASET_CSV,sep='|',error_bad_lines=False,header=None)\n",
    "df[0] = pd.to_datetime(df[0])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=df[[0,3]]\n",
    "texts.columns = ['fecha', 'noticia']\n",
    "\n",
    "texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=texts.sort_values(by=['fecha'])\n",
    "texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in texts.iterrows():\n",
    "    if index==6:\n",
    "        print(row['noticia'])\n",
    "        print(row['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_texts_perMonth = texts.groupby(texts.fecha.dt.to_period(\"M\")).count()\n",
    "result=n_texts_perMonth['noticia']\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = result.plot.bar(x='fecha', y='noticia', rot=90, color=(0.2, 0.4, 0.6, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Preprocesamiento con spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=texts.head(5)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "noticias = []\n",
    "\n",
    "for index,row in dataset.iterrows():\n",
    "    noticia = []\n",
    "    doc=nlp(row['noticia'].lower())\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.is_digit and not token.like_num:\n",
    "            noticia.append(token.lemma_)\n",
    "    noticias.append(noticia)\n",
    "    \n",
    "print(noticias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Vectorización con Gensim\n",
    "\n",
    "Gensim: https://radimrehurek.com/gensim/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(noticias)\n",
    "print(dictionary.token2id)\n",
    "#asigna un id por cada palabra del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformamos el dataset en un representacion vectorial tipo bag of word\n",
    "dataset_vectorized = [dictionary.doc2bow(noticia) for noticia in noticias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Representar la relevancia de cada palabra con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir el BOW en una representación TF-IDF\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(dataset_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in tfidf[dataset_vectorized]:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clasificación de textos básica aplicada al análisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos importando las librerías que necesitaremos para esta tarea. Ya hemos importado spaCy, pero también necesitaremos pandas y scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar un conjunto de datos real: un conjunto de reseñas de productos de Amazon Alexa. Este conjunto de datos viene como un archivo separado por tabulaciones (.tsv). Tiene cinco columnas: \n",
    "- __rating__: se refiere a la calificación que cada usuario dio a Alexa (de 0 a 5). \n",
    "- __fecha__: fecha de la reseña\n",
    "- __variación__: describe el modelo de producto Alexa que el usuario comentó.\n",
    "- __verified_reviews__: contiene el texto del comentario.\n",
    "- __feedback__: contiene un label, 0 o 1, que indica el sentimiento general negativo (0) o positivo (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TSV file\n",
    "df_amazon = pd.read_csv (\"amazon_alexa.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date         variation  \\\n",
       "0       5  31-Jul-18  Charcoal Fabric    \n",
       "1       5  31-Jul-18  Charcoal Fabric    \n",
       "2       4  31-Jul-18    Walnut Finish    \n",
       "3       5  31-Jul-18  Charcoal Fabric    \n",
       "4       5  31-Jul-18  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  \n",
       "0                                      Love my Echo!         1  \n",
       "1                                          Loved it!         1  \n",
       "2  Sometimes while playing a game, you can answer...         1  \n",
       "3  I have had a lot of fun with this thing. My 4 ...         1  \n",
       "4                                              Music         1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 records\n",
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataframe\n",
    "df_amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      "rating              3150 non-null int64\n",
      "date                3150 non-null object\n",
      "variation           3150 non-null object\n",
      "verified_reviews    3150 non-null object\n",
      "feedback            3150 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 123.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# View data information\n",
    "df_amazon.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2893\n",
       "0     257\n",
       "Name: feedback, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedback Value count\n",
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocesamiento con spacy__:\n",
    "\n",
    "Crearemos una función personalizada <code>spacy_tokenizer()</code> que acepta una frase como entrada y la procesa en tokens, realizando lemmatización, minúsculas y eliminando palabras stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorización de los textos en BoW o TF-IDF, con scikit-learn\n",
    "\n",
    "Podemos generar una matriz BoW para nuestros datos de texto usando la clase <code>CountVectorizer</code> de scikit-learn. En el código de abajo, le decimos a CountVectorizer que use la función personalizada spacy_tokenizer que construimos como su tokenizer, y que defina el rango de ngramo que queremos.\n",
    "\n",
    "Los N-gramas son combinaciones de palabras adyacentes en un texto dado, donde _n_ es el número de palabras que se incluyen en las fichas. Por ejemplo, en la frase \"¿Quién ganará la Copa del Mundo de fútbol en 2022? Bigramas sería una secuencia de dos palabras contiguas como \"quién ganará\", \"ganará la\", y así sucesivamente. Así que el parámetro ngram_range que usaremos en el código de abajo establece los límites inferior y superior de nuestros ngramas (usaremos unigramas). Entonces asignaremos los ngramas a bow_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podriamos también transformar los textos en vectores para tener los pesos TF-IDF de cada palabra en cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partición de los datos en conjuntos de entrenamiento y test para entrenar y evaluar un modelo predictivo\n",
    "\n",
    "Usaremos la mitad de nuestro conjunto de datos como nuestro conjunto de entrenamiento, que incluirá las respuestas correctas. Luego probaremos nuestro modelo usando la otra mitad del conjunto de datos sin darle las respuestas, para ver con qué precisión funciona.\n",
    "\n",
    "Convenientemente, scikit-learn nos da una función incorporada para hacer esto: train_test_split(). Sólo necesitamos decirle el conjunto de características que queremos que se divida (X), las etiquetas contra las que queremos que se realice la prueba (ylabels), y el tamaño que queremos usar para el conjunto de pruebas (representado como un porcentaje en forma decimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de un pipeline y generación del modelo\n",
    "\n",
    "Es el momento de construir nuestro modelo predictivo. Empezaremos importando el módulo LogisticRegression y creando un objeto clasificador LogisticRegression.\n",
    "\n",
    "__Para revisar sobre qué aprende y cómo aprendre el algorítmo de regresión logística: [slides](https://docs.google.com/presentation/d/11O3ud6ywHuaro6OemhyeH07nuJtdc4ybMuTJicnMnm8/edit?usp=sharing)__\n",
    "\n",
    "Luego, crearemos un pipeline de procesamiento con dos componentes: un vectorizador y algoritmo de clasificación basado en la regresión logística. El vectorizador utiliza preprocesamientos (spacy) y vectorización (scikit-learn) para crear una matriz para representar nuestros textos.\n",
    "\n",
    "Una vez que se construya este pipeline, se aprende el modelo predictivo llamando el método <code>fit()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvernier/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "       ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([('vectorizer', bow_vector),\n",
    "                 ('regression', modelLR)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación del modelo\n",
    "\n",
    "Podemos evaluar el rendimiento de nuestro modelo usando el módulo de métricas de scikit-learn. Ahora que hemos entrenado nuestro modelo, pondremos nuestros datos de prueba a disposición para hacer predicciones. Luego usaremos varias funciones del módulo de métricas para ver la exactitud, precisión y recall de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9365079365079365\n",
      "Logistic Regression Precision: 0.9399477806788512\n",
      "Logistic Regression Recall: 0.994475138121547\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  35   92]\n",
      " [   8 1440]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.28      0.41       127\n",
      "           1       0.94      0.99      0.97      1448\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1575\n",
      "   macro avg       0.88      0.64      0.69      1575\n",
      "weighted avg       0.93      0.94      0.92      1575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluación del rendimiento del clasificador\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted)\n",
    "print(confusion_matrix)\n",
    "#Print de la matriz de confusión\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, model, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(model.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 best: \n",
      "(-1.8587940443180144, 'return')\n",
      "(-1.7720516392036485, 'terrible')\n",
      "(-1.3972874278804126, 'speak')\n",
      "(-1.308530626991016, 'poor')\n",
      "(-1.1576785392789588, 'stop')\n",
      "(-1.1216336451859568, 'everytime')\n",
      "(-1.0845770867888054, 'unplug')\n",
      "(-1.0193543592310361, 'slow')\n",
      "(-0.9859931416364265, 'connect')\n",
      "(-0.9841431637665323, 'realize')\n",
      "(-0.9703184260776019, 'awful')\n",
      "(-0.9534396581777029, 'wrong')\n",
      "(-0.9082027609092996, 'half')\n",
      "(-0.8779141464709308, 'cheap')\n",
      "(-0.8652150470047145, 'load')\n",
      "(-0.8593574362031772, 'compare')\n",
      "(-0.8406491898092224, 'turn')\n",
      "(-0.8222110940950802, 'question')\n",
      "(-0.8141047596051005, ':(')\n",
      "(-0.8102641995867899, 'plug')\n",
      "Class 2 best: \n",
      "(2.336059757114147, 'love')\n",
      "(1.6201039559950892, 'easy')\n",
      "(1.5568527690502412, 'great')\n",
      "(1.0275174231081405, 'fun')\n",
      "(0.9632324313827056, 'enjoy')\n",
      "(0.9338020587948, 'amaze')\n",
      "(0.9006410954535663, 'good')\n",
      "(0.8554223428614259, 'awesome')\n",
      "(0.8344532401006808, 'perfect')\n",
      "(0.8060263157392237, 'listen')\n",
      "(0.7984323149028595, 'expect')\n",
      "(0.7466490516517258, 'room')\n",
      "(0.7262044762846049, 'purchase')\n",
      "(0.7254701225669782, 'light')\n",
      "(0.7177779702932803, 'smart')\n",
      "(0.7067275907279691, 'command')\n",
      "(0.6780070073731608, 'use')\n",
      "(0.6469992519564699, 'speaker')\n",
      "(0.6416052079966732, 'excellent')\n",
      "(0.628703262135409, 'nice')\n"
     ]
    }
   ],
   "source": [
    "printNMostInformative(bow_vector, modelLR, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejercicios 'simples' para practicar\n",
    "\n",
    "Datasets: [latercera0.csv](https://drive.google.com/file/d/18uxulc23NQTVAjUKIygAZI3WYpeKsrBh/view?usp=sharing), [lacuarta1.csv](https://drive.google.com/file/d/1YyLVD-WD1oNtUA50dR0QgYLirwzuq12G/view?usp=sharing), [eldesconcierto2.csv]()\n",
    "\n",
    "1. Mostrar los 10, 20 y 50 sustantivos más frecuentes en el medio La Tercera. Tratar de visualizar la distribución de estos sustantivos en un histograma ordenado.\n",
    "1. ¿Cómo se distribuyen las categorías gramaticales (POS tags) en La Tercera? ¿Se diferencia de otros medios?\n",
    "1. ¿Cuáles son las palabras más frecuentes en cada medio (filtrando los stop-words)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ejercicio: clasificar noticias de prensa en 2 temáticas\n",
    "\n",
    "- Pregunta de investigación: ¿podemos aprender un modelo capaz de reconocer el estílo de cada medio (latercera vs. lacuarta, latercera vs. eldesconscierto, o lacuarta vs. eldesconcierto)\n",
    "\n",
    "Aprender un modelo para clasificar una noticia según su fuente probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
